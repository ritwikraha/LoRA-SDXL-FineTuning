{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwikraha/LoRA-SDXL-FineTuning/blob/main/notebooks/Image_maker_LoRA_w_LLava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Style Creator with SDXL LoRA Adapter\n",
        "\n"
      ],
      "metadata": {
        "id": "FKKcBGDbZgXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Installations\n",
        "\n",
        "NOTE: Make sure to install `diffusers` from `main` .Download diffusers SDXL DreamBooth training script."
      ],
      "metadata": {
        "id": "oXt6C-nBa9Wo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUxRrLfLMnBb"
      },
      "outputs": [],
      "source": [
        "# Check the GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55U57alsRIek"
      },
      "outputs": [],
      "source": [
        "# Install dependencies.\n",
        "!pip install bitsandbytes transformers accelerate peft -q\n",
        "!pip install git+https://github.com/huggingface/diffusers.git -q\n",
        "!pip install datasets -q\n",
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_sdxl.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from google.colab import files\n",
        "from huggingface_hub import snapshot_download\n",
        "from PIL import Image\n",
        "import glob\n",
        "import requests\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "import torch\n",
        "import json\n",
        "import locale\n",
        "from huggingface_hub import whoami\n",
        "from pathlib import Path\n",
        "\n",
        "from train_dreambooth_lora_sdxl import save_model_card\n",
        "from huggingface_hub import upload_folder, create_repo\n",
        "from huggingface_hub import upload_file\n",
        "from diffusers import DiffusionPipeline, AutoencoderKL\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "bXNjNAlSBZWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "UP_0OSiDB4TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_name= \"comics\""
      ],
      "metadata": {
        "id": "4wzKTUkCbVrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBcvYAyS8bU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UNkbXwj126Q"
      },
      "source": [
        "We will be uploading custom images and using the LLava processor to caption them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqqhZ9R9x3mG"
      },
      "outputs": [],
      "source": [
        "# pick a name for the image folder\n",
        "local_dir = \"./\"+concept_name+\"/\"\n",
        "os.makedirs(local_dir)\n",
        "os.chdir(local_dir)\n",
        "\n",
        "# choose and upload local images into the newly created directory\n",
        "uploaded_images = files.upload()\n",
        "os.chdir(\"/content\") # back to parent directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piJkbOe9OZbX"
      },
      "source": [
        "Preview the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0R-hByAPkIg"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols, resize=256):\n",
        "\n",
        "    if resize is not None:\n",
        "        imgs = [img.resize((resize, resize)) for img in imgs]\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change path to display images from your local dir\n",
        "img_paths = \"./\"+concept_name+\"/*.png\"\n",
        "imgs = [Image.open(path) for path in glob.glob(img_paths)]\n",
        "\n",
        "num_imgs_to_preview = 10\n",
        "image_grid(imgs[:num_imgs_to_preview], 1, num_imgs_to_preview)"
      ],
      "metadata": {
        "id": "_OB5Cjfhiiyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqrxnDMUTADH"
      },
      "source": [
        "### Generate custom captions with LLava\n",
        "Load LLava to auto caption your images:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "llava_processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# captioning utility\n",
        "def caption_images(prompt, input_image):\n",
        "    inputs = llava_processor(prompt, images=input_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "    # pixel_values = inputs.pixel_values\n",
        "\n",
        "    generated_ids = llava_model.generate(**inputs, max_length=60, do_sample=False)\n",
        "    generated_caption = llava_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Split the caption by line breaks\n",
        "    lines = generated_caption.splitlines()\n",
        "    for line in lines:\n",
        "      if line.startswith(\"ASSISTANT:\"):\n",
        "        # Extract text after \": \" (without using split and indexing)\n",
        "        assistant_line = line.partition(\": \")[2]\n",
        "        if assistant_line.startswith(\"In the image,\"):\n",
        "          # Extract text after \"In the image,\" (without using split and indexing)\n",
        "          in_the_image_line = assistant_line.partition(\"In the image,\")[2]\n",
        "          # print(in_the_image_line)\n",
        "          break  # Exit the loop after finding the first assistant line\n",
        "    return in_the_image_line"
      ],
      "metadata": {
        "id": "RnJL0sWyTnoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfkwE439x3mH"
      },
      "outputs": [],
      "source": [
        "# create a list of (Pil.Image, path) pairs\n",
        "local_dir = \"./\"+concept_name+\"/\"\n",
        "imgs_and_paths = [(path,Image.open(path)) for path in glob.glob(f\"{local_dir}*.png\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V9cYeVDx3mH"
      },
      "source": [
        "Now let's add the concept token identifier (e.g. TOK) to each caption using a caption prefix.\n",
        "Change the prefix to the concept you are training on\n",
        "- for this example we can use \"a photo of TOK,\" other options include:\n",
        "    - For styles - \"In the style of TOK\"\n",
        "    - For faces - \"photo of a TOK person\"\n",
        "- You can add additional identifiers to the prefix that can help steer the model in the right direction.\n",
        "-- e.g. for this example, instead of \"a photo of TOK\" we can use \"a photo of TOK comics\" / \"In the style of TOK comics\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJcZyeSVx3mH"
      },
      "outputs": [],
      "source": [
        "caption_prefix = \"in the style of TOK \"+concept_name+\", \"\n",
        "prompt_to_captioner = \"USER: <image>\\nWhat is happening in this image?\\nASSISTANT:\"\n",
        "with open(f'{local_dir}metadata.jsonl', 'w') as outfile:\n",
        "  for img in imgs_and_paths:\n",
        "      caption = caption_prefix + caption_images(prompt_to_captioner,img[1]).split(\"\\n\")[0]\n",
        "      entry = {\"file_name\":img[0].split(\"/\")[-1], \"prompt\": caption}\n",
        "      json.dump(entry, outfile)\n",
        "      outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_5Skm_cx3mI"
      },
      "source": [
        "Free some memory for the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4wfEW_cx3mI"
      },
      "outputs": [],
      "source": [
        "# delete the BLIP pipelines and free up some memory\n",
        "del llava_processor, llava_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLptJV1vx3mI"
      },
      "source": [
        "## Prepare for Training\n",
        "\n",
        "Initialize `accelerate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW7eFPl4eYXy"
      },
      "outputs": [],
      "source": [
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc0bxTr918es"
      },
      "source": [
        "### Log into your Hugging Face account\n",
        "Pass [your **write** access token](https://huggingface.co/settings/tokens) so that we can push the trained checkpoints to the Hugging Face Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hf07sapecFY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-p7R70THc5"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az-IQG-Ax3mJ"
      },
      "source": [
        "#### Set Hyperparameters\n",
        "To ensure we can DreamBooth with LoRA on a heavy pipeline like Stable Diffusion XL, we're using:\n",
        "\n",
        "* Gradient checkpointing (`--gradient_accumulation_steps`)\n",
        "* 8-bit Adam (`--use_8bit_adam`)\n",
        "* Mixed-precision training (`--mixed-precision=\"fp16\"`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbz4IXe0bAn"
      },
      "source": [
        "### Launch training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMwkmL7N82R"
      },
      "source": [
        "To allow for custom captions we need to install the `datasets` library, you can skip that if you want to train solely\n",
        " with `--instance_prompt`.\n",
        "In that case, specify `--instance_data_dir` instead of `--dataset_name`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Use `--output_dir` to specify your LoRA model repository name!\n",
        " - Use `--caption_column` to specify name of the cpation column in your dataset. In this example we used \"prompt\" to\n",
        " save our captions in the\n",
        " metadata file, change this according to your needs."
      ],
      "metadata": {
        "collapsed": false,
        "id": "fbri7qQ0mdyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOiZSXHfx3mJ"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env bash\n",
        "!accelerate launch train_dreambooth_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
        "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
        "  --dataset_name=\"comics\" \\\n",
        "  --output_dir=\"comics_style_LoRA\" \\\n",
        "  --caption_column=\"prompt\"\\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --instance_prompt=\"an illustration in the style of TOK comics\" \\\n",
        "  --resolution=1024 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=3 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=1e-4 \\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --max_train_steps=500 \\\n",
        "  --checkpointing_steps=717 \\\n",
        "  --seed=\"97\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11tgWz5x3mK"
      },
      "source": [
        "### Save the model to the hub and check it out\n",
        "\n",
        "NOTE: make sure the `output_dir` you specify here is the same as the one used for training\n",
        "\n",
        "Sometimes training finishes succesfuly (i.e. a **.safetensores** file with the LoRA weights saved properly to your local `output_dir`) but there's not enough RAM in the free tier to push the model to the hub 🙁\n",
        "To mitigate this, run this cell with your training arguments to make sure your model is uploaded! 🤗\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uyy9n54EtBY"
      },
      "outputs": [],
      "source": [
        "output_dir = concept_name+\"_style_LoRA\"\n",
        "username = whoami(token=Path(\"/root/.cache/huggingface/\"))[\"name\"]\n",
        "repo_id = f\"{username}/{output_dir}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# push to the hub🔥\n",
        "repo_id = create_repo(repo_id, exist_ok=True).repo_id"
      ],
      "metadata": {
        "id": "--khEQSvsZYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(repo_id)"
      ],
      "metadata": {
        "id": "PCAPpNqQscPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token=\"\""
      ],
      "metadata": {
        "id": "4neOmeLaNX3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smLheRkvEQ9H"
      },
      "outputs": [],
      "source": [
        "# change the params below according to your training arguments\n",
        "save_model_card(\n",
        "    repo_id = repo_id,\n",
        "    images=[],\n",
        "    use_dora=False,\n",
        "    base_model=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    train_text_encoder=False,\n",
        "    instance_prompt=\"a photo in the style of TOK\"+concept_name,\n",
        "    validation_prompt=None,\n",
        "    repo_folder=output_dir,\n",
        "    vae_path=\"madebyollin/sdxl-vae-fp16-fix\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    token=token,\n",
        "    folder_path=output_dir,\n",
        "    commit_message=\"End of training\",\n",
        "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Jm9JZxqMtulT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLoRqZGux3mK"
      },
      "outputs": [],
      "source": [
        "link_to_model = f\"https://huggingface.co/{repo_id}\"\n",
        "display(Markdown(\"### Your model has finished training.\\nAccess it here: {}\".format(link_to_model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH7-YJwMcyra"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTz6Zmfc0i-0"
      },
      "outputs": [],
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        "    use_safetensors=True\n",
        ")\n",
        "pipe.load_lora_weights(repo_id)\n",
        "_ = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J1NsUP51E2w"
      },
      "outputs": [],
      "source": [
        "prompt = \"some generic but specialized prompt, 8k\"\n",
        "\n",
        "image = pipe(prompt=prompt, num_inference_steps=50).images[0]\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}